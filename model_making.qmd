---
output: html_document
editor_options: 
  chunk_output_type: console
---
#Libraries
```{r}
library(tidyverse)
library(lubridate)
library(recipes)
library(themis)
library(vip)
library(rpart)
library(recipes)
library(parsnip)
library(glmnet)
library(ranger)
library(kknn)
library(rsample)
library(tidymodels)
```

#Data Split and Folds
```{r}
data <- read.csv("final_lead_data_weather.csv")

data$weekday <- wday(data$date, label = TRUE)
data$month <- month(data$date, label = TRUE)
data$yearday <- yday(data$date)

bike_modeling <- data |>
  slice(1:1379)

bike_implementation <- data |>
  slice(1380:1409) |>
  select(-ridership)

set.seed(20211101)

data_split <- initial_split(bike_modeling)
data_train <- training(data_split)
data_test <- testing(data_split)

data_folds <- vfold_cv(data = data_train, v = 10, repeats = 5)
```

#Recipe
```{r}
#Recipe
bike_rec <- recipe(ridership ~ ., data = data_train) |>
  update_role(date, new_role = "id") |>
  step_zv(all_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_nzv(all_predictors()) |>
  step_lincomb(all_numeric_predictors()) |>
  step_corr(all_numeric_predictors(), threshold = 0.9) |>
  step_normalize(all_predictors())

```

#Linear Model
```{r}
#Linear Model
#Can we hyperparamterize this?

lm <- linear_reg() |>
  set_mode(mode = "regression") |>
  set_engine(engine = "lm")

lm_wf <- workflow() |>
  add_model(lm) |>
  add_recipe(bike_rec)

lm_res <- lm_wf |>
  fit_resamples(resamples = data_folds, rankdeficients = "NA")

lm_metrics <- lm_res |>
  collect_metrics(summarize = FALSE) |>
  filter(.metric == "rmse")
```

#KNN Model
```{r}
#KNN
knn <-  nearest_neighbor(neighbors = tune()) |>
  set_engine(engine = "kknn") |>
  set_mode(mode = "regression")

knn_grid <- grid_regular(neighbors(range = c(1, 99)), levels = 10)

knn_wf <- workflow() |>
  add_recipe(bike_rec) |>
  add_model(knn)

knn_res <- knn_wf |>
  tune_grid(
    resamples = data_folds,
    grid = knn_grid,
    metrics = metric_set(rmse)
  )

knn_wf_final <- knn_wf |> 
  finalize_workflow(select_best(knn_res))

#Think this isn't right
#knn_res_final <- knn_wf_final |>
#  tune_grid(
#    resamples = data_folds,
#    grid = knn_grid,
#    metrics = metric_set(rmse) )

knn_res_final <- knn_wf_final |>
  last_fit(data_split) 

knn_metrics_final <- knn_res_final |>
  collect_metrics(summarize = FALSE) |>
  filter(.metric == "rmse")

```

#RF Model
```{r}
rf <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = 500
) |>
  set_mode("regression") |>
  set_engine("ranger",
             importance = "impurity")

rf_wf <- workflow() |>
  add_model(rf) |>
  add_recipe(bike_rec)

rf_grid <- grid_regular(
  mtry(range = c(1, 10)),
  min_n(range = c(1, 20)),
  levels = 3
)

rf_resample <- tune_grid(
  rf_wf, 
  resamples = data_folds,
  grid = rf_grid,
  metrics = metric_set(rmse)
)

#rm_metrics <- rf_resample |>
 # collect_metrics(summarize = FALSE) |>
  #filter(.metric == "rmse")

best_rf <- select_best(
  rf_resample,
  metric = "rmse"
)

rf_wf_final <- rf_wf |>
  finalize_workflow(best_rf)

rf_res_final <- rf_wf_final |>
  last_fit(data_split)

rf_metrics_final <- rf_res_final |>
  collect_metrics(summarize = FALSE) |>
  filter(.metric == "rmse")

rf_mean_rmse  <- mean(rf_metrics_final$.estimate)

```

#Ridge Model
```{r}
#Ridge

 ridge_mod <- linear_reg(penalty = 30, mixture = 0) |>
    set_mode(mode = "regression") |>
    set_engine(engine = "glmnet")  
  
  ridge_wf <- workflow() |>
    add_recipe(recipe = bike_rec) |>
    add_model(spec = ridge_mod)

  ridge_resamples <- fit_resamples(ridge_wf, resamples = data_folds)
  
  
  ridge_rmse <- collect_metrics(ridge_resamples) |>
  filter(.metric == "rmse")

  ridge_mean_rmse <- mean(ridge_rmse$mean)

```
#Regression Trees
```{r}
#can we fiddle with cost_complexity?
dt_mod <- decision_tree(cost_complexity = 0.001) |>
  set_mode(mode = "regression") |>
  set_engine(engine = "rpart")

dt_wf <- workflow() |>
  add_recipe(bike_rec) |>
  add_model(dt_mod)

dt_res <- dt_wf |>
  fit_resamples(resamples = data_folds)

dt_metrics <- dt_res |>
  collect_metrics(summarize = FALSE) |>
  filter(.metric == "rmse")

```
#LASSO
```{r}
lasso_mod <- linear_reg(penalty = tune(), mixture = 1) |>
  set_mode(mode = "regression") |>
  set_engine(engine = "glmnet")
  
lasso_wf <- workflow() |>
    add_recipe(recipe = bike_rec) |>
    add_model(spec = lasso_mod)

lasso_grid <- grid_regular(penalty(range = c(-4, 2)), levels = 30)
  
lasso_tune_res <- tune_grid(
  lasso_wf,
  resamples = data_folds,
  grid = lasso_grid
)
  
lasso_metrics <- lasso_tune_res |>
  collect_metrics() |>
  filter(.metric == "rmse")
  
```


#Bagging



#RMSEs
```{r}
# Calculate RMSE
lm_mean_rmse  <- mean(lm_metrics$.estimate)
knn_mean_rmse <- mean(knn_metrics_final$.estimate)
rf_mean_rmse  <- mean(rf_metrics_final$.estimate)
ridge_mean_rmse <- mean(ridge_rmse$mean)
dt_mean_rmse <- mean(dt_metrics$.estimate)
lass_mean_rmse <- mean(lasso_metrics$mean)
```

#RMSE Visualizations
```{r}

ggplot(lm_metrics, aes(x = id, y = .estimate)) +
  geom_point() +
  labs(title = "RMSE across folds â€“ Linear regression",
       x = "Fold", y = "RMSE") +
  theme_minimal()
```

```{r}
## Estimate the out-of-sample error rate?
```